{
  "hash": "510b116598963684e1fb1574b0833512",
  "result": {
    "markdown": "---\nauthor: Rohan Sharma\ndate: 10/24/2023\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n    toc: true\n    mermaid:\n      theme: default\ntitle: Neural Networks From Scratch -- ML practitioner's guide\ncategories:\n  - Deep Learning\n  - Neural Networks\n  - Pytorch\n---\n\n![](img_nn.png)\n\n---\n\nAn ideal read for ML practitioners venturing into Neural Networks for the first time. A basic tutorial of implementing \nNeural Networks from scratch, bridging the gap between the reader's ML knowledge and the Neural Network concepts. \n The time is ripe to enhance your ML expertise!\n\n\n\nA few days back I started learning Neural Networks, before that I had a basic knowledge of Machine Learning. \nSo, I think this is the best time to write this article for those who know ML and are hoping to start learning Neural Networks. I have found these techniques useful to learn NN from the mind of a person who knows ML priorly.\nI have explained the concepts in a way that you can relate them with the concepts of ML and it will be a very quick journey for you to learn these concepts quickly and easily. \n\nThis post will guide you through implementing a basic neurons, then combining those neurons to form layers and finally combining layers to make Neural Networks. \n\n# Let's get Started ! üèÅ\n\nTo start this awesome journey, let's first quickly revise some concepts of ML... this is the time to recall them and prepare your mind to get started. \n\n### ML recall  üí°\n\nThe only concept that you need to remember to get this article in your is of **Linear Regression!**. \n\nHere are the steps that we follow in Linear Regression : \n\n\n1. Get a tabular that we want to train.\n2. Saperate the data in two parts, *Independent* and *Dependent Variables*. \n   $$[x_1^1, x_2^1, x_3^1, x_4^1, x_5^1...] , [y_1]$$\n   $$[x_1^2, x_2^2, x_3^2, x_4^2, x_5^2...] , [y_2]$$\n   $$[x_1^3, x_2^3, x_3^3, x_4^3, x_5^3...] , [y_3]$$\n   $$.....$$\n   \n3. The equation to fit the line is ..\n   \n   $$y = mX + b$$\n4. Loss Function used can be root mean square error...\n   $$RMSE = \\sum_{i=1}^{D}(y_i-(mX_i + b))^2$$\n6. Now we find a best-fit line that fits the data with minimum loss. We do this with the help of gradient descent (reducing the loss in every step by modifying the weights)\n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(5,4))\nplt.scatter(x,y, label='data points')\nfrom sklearn.linear_model import LinearRegression\npred = LinearRegression().fit(x,y).predict(x)\nplt.plot(x,pred, color='orange', linewidth=5, label='best-fit')\n# plt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![linear regression](index_files/figure-html/fig-limit-output-1.png){#fig-limit width=418 height=337}\n:::\n:::\n\n\nAnd so, this is all you need to know to get started ! So, now lets dive deep into implementation of NN from scratch. \n\n## Know some Tensors üçÉ\n\nTensors are a fundamental data structure, very similar to arrays and matrices.\nA tensor can be represented as a matrix, but also as a vector, a scalar, or a higher-dimensional array. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom torch import tensor\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnp_array = np.array([[2,5,7,2],[2,6,8,3]])\none_d = tensor([1,4,5,6])\ntwo_d = tensor(np_array)\nprint(one_d); print(two_d);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1, 4, 5, 6])\ntensor([[2, 5, 7, 2],\n        [2, 6, 8, 3]], dtype=torch.int32)\n```\n:::\n:::\n\n\n`tensor` are just like `numpy arrays` and so, there is not much to think about them, lets get to the next **big** thing...\n\n# Designing your first neuron ‚öõÔ∏è\n\nA **single neuron (or perceptron)** is the basic unit of a neural network... \n\nIt takes <u>*input*</u> as <u>*dependent variables*</u>, multiplies it with the <u>*weights*</u>, adds the <u>*bais*</u> , goes through an <u>*activation function*</u>, and produces the <u>*output*</u>. \n\nI know what you are thinking now üëÄ... \n\n1. Huh! It looks the same as Linear Regression Model !!\n\n    üòå Yes, it is just another Linear Regression .. \n2. Then what is that *activation function*?\n   \n    üòå This is something you will get to know in the near future.\n\n3. Should I relax?\n\n   üòå Absolutely NOT !!\n\n\n![A Perceptron](img.png)\nSo, Let's implement this using `tensor`\n\n###  Data Preprocessing (can be skipped)\n\nThe data used for this eg is the most famous, Titanic Dataset. Some preprocessing steps are done (you might be familiear with them and so, they can be skipped)\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Some Data Cleaning and Preprocessing\n\ndf = pd.read_csv('train.csv')  # read dataset\n\nmode = df.mode(axis=0,).iloc[0]  # calculating mode of all columns\ndf.fillna(mode, inplace=True)   # filling missing values with mode\n\ndf['FareLog'] = np.log(df['Fare'] + 1)    # taking log of fare ( feature engineering )\n\ncat_cols = ['Sex', 'Pclass', 'Embarked']  # category columns\ndf_cat = pd.get_dummies(df, columns=cat_cols)  # transforming categories to numerical values\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'FareLog', \n        'Pclass_1', 'Pclass_2', 'Pclass_3',\n       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'Embarked_S']     # list of independent columns \ndep_cols = ['Survived']   # list of dependent columns\n\ndf = df_cat[indep_cols + dep_cols]   # final dataset\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf.head(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>FareLog</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Embarked_S</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.110213</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4.280593</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Set X and Y tensors\n\n\nThe `Survived` value is what we have to predict. So, here instead of using `pandas` or `numpy`, we are using `tensor`!! \n\nInitialize the values of dependent and independent variables as tensors like this...\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nX = tensor(df[indep_cols].values.astype(float))\ny = tensor(df[dep_cols].values.astype(float))\nmax_val, idxs = X.max(dim=0)\nX = X / max_val   \n```\n:::\n\n\n::: {.cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nX.shape :  torch.Size([891, 15])\ny.shape :  torch.Size([891, 1])\n\nFirst Row of X :\ntensor([0.2750, 0.1250, 0.0000, 0.3381, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n        0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000], dtype=torch.float64)\n\nFirst Row of y :\ntensor([0.], dtype=torch.float64)\n```\n:::\n:::\n\n\n#### Initialize the weights\nWe have to make a weights tensor that will be used as *coefficients*. \nAt the start of training, it can be a random valued tensor, we can design a function for this.. To keep it simple initially, we are not using bais term rn.\n\n`torch.rand()` takes the shape of required tensor and generates it \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef init_coeff(indep_var):\n    n_coeffs = indep_var.shape[1]\n    return (torch.rand(n_coeffs) - 0.5)\n```\n:::\n\n\n` - 0.5 ` is done to normalize the values\n\n\n\n#### Calculate Predictions and Loss\nVery similar to Linear Regression, Predictions can be calculated by the *weighted sum* of *features* and *weights*.\nFor *loss* we have used *Mean Absolute Error*..\n\nLet's make a function for this... \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef calc_preds(coeff, indep_var):\n    return (indep_var*coeff).sum(axis=1)[:,None] \n    \ndef calc_loss(coeff, indep_var, dep_var):\n    return torch.abs(calc_preds(coeff, indep_var) - dep_var).mean()\n```\n:::\n\n\n#### Epochs \nNow,as we have all the necessary functions, we can perform our first epoch, a **single epoch** will calculate the loss, takes the gradient value according to learning rate and modify the coefficients to reduce the error. This is exactly same as a single step of gradient descent...\n\n$$Weight = Weight - LearningRate * Gradient$$\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef update_coeff(coeff, lr):\n    # subtracts the value from coeff and save it as a new value\n    coeff.sub_(coeff.grad * lr)     \n    coeff.grad.zero_()         # sets the coeff.grad value to 0\n```\n:::\n\n\n```{mermaid}\nflowchart LR\n  A[Random Weight] --> B(Calculate \\nLoss)\n  B --> C[Calculate \\nGradient] --> D[Modify\\n Weights] -- \" for n epochs \" --> B\n  D --> E[Final Weights]\n```\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef one_epoch(coeff, lr):\n    loss = calc_loss(coeff, X, y)\n    loss.backward()                                 # new term alert !\n    print(f' loss : {loss:.3} ', end=';')\n    with torch.no_grad():                         # no_grad mode\n        update_coeff(coeff, lr)\n   \n\ndef train_model(n_epochs, lr):\n    torch.manual_seed(442)\n    coeff = init_coeff(X)\n    coeff.requires_grad_()                         # new term alert !\n    for i in range(n_epochs):\n        one_epoch(coeff, lr)\n    print('\\n\\nFinal Loss: ', float(calc_loss(coeff, X, y)))\n    return coeff\n```\n:::\n\n\n### Autograd Machenism ‚ö†Ô∏è\n`coeff.requires_grad_()` sets the value of `coeff.requires_grad` = `True`... when this is set true, the gradients values are computed for these tensors, which can be used afterward in backpropagation. So, when a *back pass* is done, their `.grad` values update with new gradient values. \n\n\n`loss.backward()` is a *back pass* here. When calculations are performed (in forward pass), an operation is recorded in backward graph when at least one of its input tensors requires gradient. And when we call `loss.backward()`, leaf tensors with `.requires_grad = True` will have gradients accumulated to their `.grad` fields.\n\nSo, We calculate gradients by forward and backward propagation. We first set the `coeff.requires_grad = True` then the loss is calculated for the `coeff`. As coeff requires grad so, this operation will be added in backward graph. And now, when we call `loss.backward()`, it calculates the `.grad` values of all those tensors which requires grad and are in the backward graph. So, the coeff tensor will have its `.grad` value updated. \n\nThen we update the `coeff` value. While doing this, we dont want this operation to be saved in backward graph so, we keep this modification under `torch.no_grad()` function, and so, this operation is neglected and is not tracked. Next when again loss is calculated and backward fun. called, new grad values gets updated.... and the process carries on.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ncoeff = train_model(40,0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n loss : 0.718 ; loss : 0.68 ; loss : 0.658 ; loss : 0.638 ; loss : 0.618 ; loss : 0.599 ; loss : 0.581 ; loss : 0.562 ; loss : 0.543 ; loss : 0.525 ; loss : 0.506 ; loss : 0.488 ; loss : 0.469 ; loss : 0.451 ; loss : 0.433 ; loss : 0.415 ; loss : 0.397 ; loss : 0.379 ; loss : 0.362 ; loss : 0.345 ; loss : 0.33 ; loss : 0.317 ; loss : 0.306 ; loss : 0.296 ; loss : 0.289 ; loss : 0.29 ; loss : 0.287 ; loss : 0.304 ; loss : 0.268 ; loss : 0.279 ; loss : 0.276 ; loss : 0.291 ; loss : 0.274 ; loss : 0.288 ; loss : 0.276 ; loss : 0.285 ; loss : 0.28 ; loss : 0.285 ; loss : 0.283 ; loss : 0.287 ;\n\nFinal Loss:  0.2839040984606188\n```\n:::\n:::\n\n\n### Activation Function üç≠\n\nHere we have used another function `torch.sigmoid()` over the calculated predictions, which is known as **Activation Function!**.\nWe know that the weighted sum can produce the values less than 0 or more than 1. But our `y` has values only between 0 and 1. so, to restrict the values, we use the *sigmoid* function... which you might be familiear with if you know Logistic Regression. \n\n$$\\sigma(z) = \\frac{1} {1 + e^{-z}}$$\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef calc_preds(coeff, indep_var):\n    reg_pred = (indep_var*coeff).sum(axis=1)[:,None]\n    return torch.sigmoid(reg_pred)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ncoeff = train_model(40,50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n loss : 0.562 ; loss : 0.33 ; loss : 0.311 ; loss : 0.307 ; loss : 0.305 ; loss : 0.304 ; loss : 0.302 ; loss : 0.3 ; loss : 0.292 ; loss : 0.234 ; loss : 0.217 ; loss : 0.215 ; loss : 0.214 ; loss : 0.212 ; loss : 0.21 ; loss : 0.206 ; loss : 0.203 ; loss : 0.201 ; loss : 0.2 ; loss : 0.199 ; loss : 0.198 ; loss : 0.197 ; loss : 0.197 ; loss : 0.196 ; loss : 0.196 ; loss : 0.196 ; loss : 0.195 ; loss : 0.195 ; loss : 0.195 ; loss : 0.195 ; loss : 0.195 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.193 ; loss : 0.193 ;\n\nFinal Loss:  0.1931743193621472\n```\n:::\n:::\n\n\n**Congratulations !! You have trained your first Neuron !!!!!**\n\n# Building a Layer of NN ü™µ\n\nA single layerd NN will just contain more than one Neurons as the layer. Each neuron will receive all features values and gives an output. The outputs of all the neurons of the layer will passed to output neuron, which will combine them to produce the final output. \n\nThe first layer neurons will have weights equal to size of input features. The output neuron will contain the weights of size of no. of neurons in first layer. \n\nSimple !!! Take a look at hidden neuron = 2 and with hidden neuron = 1\n\n```{mermaid}\nflowchart TD\n  A((1)) --> Z((F))\n  B((2)) --> Z\n  Z --> V((Z))\n  AA((I1)) --> A\n  AB((I2)) --> A\n  AB --> B\n  AC((I3)) --> A\n  AC --> B\n  AD((I4)) --> A\n  AD --> B\n  AA --> B\n  A1((1)) --> Z1((F))\n  Z1 --> V1((Z))\n  AA1((I1)) --> A1\n  AB1((I2)) --> A1\n  AC1((I3)) --> A1\n  AD1((I4)) --> A1\n```\n\nSo `I1 - I4` are input features, `1` and `2` are the neurons of first layer, `\n\nF` is the output neuron, which takes `1` and `2` as input. \n\nFinally `Z` is the activation function. \n\n**Initialization of Weights...**\n\nthe weights will contain `layer_1` of shape (n_coeff, n_hidden), so it will contain n_hidden set of weights, where the shape of each weight set will be of size n_coeff (input features).\n\nAnd `layer_2` will have 1 set of weights where the weight set will be of size n_hidden. `const` is the bais term added. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndef init_coeffs(n_hidden, indep_var):\n    n_coeff = indep_var.shape[1]\n    layer1 = ((torch.rand(n_coeff, n_hidden) - 0.5) / n_hidden).double().requires_grad_()  # shape of (n_coeff, n_hidden)\n    layer2 = ((torch.rand(n_hidden, 1)) - 0.3).double().requires_grad_()                  # output layer, shape of (n_hidden, 1)\n    const = ((torch.rand(1, 1)-0.5).double() * 0.1).requires_grad_()\n    return layer1, layer2, const\ncoeff = init_coeffs(2, X)\n```\n:::\n\n\n**Calculate Predictions and Loss**\n\nThe predictions of hidden layer are calculated by `matmul()` fun for weighted sum by matrix multiplication, the *activation function* used for first layer is **ReLu Activation Function**. It is a more simpler function, which sets any -ve value to 0. \n$$Relu(z) = max(0, z)$$\nThe output of first layer is given input for second layer and so, is multiplied with layer2 and const term is added. The final output layer uses the **sigmoid** function. \nNo change required in Loss Function\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport torch.nn.functional as F\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndef calc_preds(coeff, indep_var):\n    layer1, layer2, const = coeff\n    op1 = F.relu(indep_var.matmul(layer1))     # new term alert\n    op2 = op1.matmul(layer2) + const\n    return torch.sigmoid(op2)\n```\n:::\n\n\n**Update Coefficient** requires just a slight change, instead of single coeff, we have multiple coeff (`layer1, layer2, coeff`), so loop over all of them \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndef update_coeff(coeff, lr):\n    for layer in coeff:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n```\n:::\n\n\n**Train Function** requires a slight change to give `n_hidden` value as input for init_coeff()\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ndef train_model(n_epochs, lr, n_hidden=2):\n    coeff = init_coeffs(n_hidden, X)\n    for i in range(n_epochs):\n        one_epoch(coeff, lr)\n    print('\\n\\nFinal Loss: ', float(calc_loss(coeff, X, y)))\n    return coeff\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ncoeff = train_model(40,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n loss : 0.518 ; loss : 0.467 ; loss : 0.44 ; loss : 0.423 ; loss : 0.412 ; loss : 0.405 ; loss : 0.399 ; loss : 0.394 ; loss : 0.386 ; loss : 0.365 ; loss : 0.298 ; loss : 0.295 ; loss : 0.333 ; loss : 0.323 ; loss : 0.317 ; loss : 0.309 ; loss : 0.27 ; loss : 0.229 ; loss : 0.226 ; loss : 0.224 ; loss : 0.221 ; loss : 0.217 ; loss : 0.212 ; loss : 0.208 ; loss : 0.207 ; loss : 0.209 ; loss : 0.203 ; loss : 0.202 ; loss : 0.199 ; loss : 0.198 ; loss : 0.197 ; loss : 0.196 ; loss : 0.196 ; loss : 0.195 ; loss : 0.195 ; loss : 0.195 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ; loss : 0.194 ;\n\nFinal Loss:  0.19344465494371008\n```\n:::\n:::\n\n\n# Building a Complete Neural Network üîÆ\n\nA complete NN will contain several layers, with different sizes. Each layer will have some perceptrons, and perceptrons of a layer will have weights of size equal to no. of perceptron in previous layer. \n\nSo, there can be any number of neurons in a layer but, they all will receive ouptput of the previous layer and so their weights should match.\n\nFor eg, if a neural network has following layers : \n\n- *Input Layer* - 3 inputs\n  \n- *Hidden Layers* -\n    - Layer 1 - 2 perceptrons    ---> (3 weights in every perceptron)\n    - Layer 2 - 2 perceptrons    ---> (2 weights in each perceptron)\n\n      \n- *Output Layer* - 1 perceptron  ---> (2 weights in each perceptron)\n\n*And, so here we have a complete neural network...!!*\n\n```{mermaid}\nflowchart LR \n  A1((L1)) --> Z1((L2))\n  Z1 --> V1((Z))\n  AA1((I1)) --> A1\n  AA1 --> B((L1))\n  AB1((I2)) --> A1\n  AB1 --> B\n  AC1((I3)) --> A1\n  AC1 --> B\n  B --> Z1\n  B --> Z2\n  A1 --> Z2((L2))\n  Z2 --> V1\n  V1 --> AL(OUTPUT)\n  \n```\n\n**Weights Initialization** can contain a `list` of layers size as input and so it will return the coefficients and bais matrix for all layers. \n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef init_coeffs(n_hidden: list, indep_var):\n    n_coeff = indep_var.shape[1]\n    sizes = [n_coeff] + n_hidden + [1]    # inputs, hidden, output\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1]).double() - 0.5)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [((torch.rand(1,1) - 0.5) * 0.1).double() for i in range(n-1)]\n    for l in (layers + consts) : \n        l.requires_grad_()\n    return layers, consts\n```\n:::\n\n\n`- 0.5`, `* 0.1`, `* 4`... and all these we only do for initial setup so that the random coeff. generated can be close to the optimum value in the start, (helpful for quickly converging the loss)\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n[x.shape for x in init_coeffs([4, 2], X)[0]]\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\n[torch.Size([15, 4]), torch.Size([4, 2]), torch.Size([2, 1])]\n```\n:::\n:::\n\n\nAnd here we can first layer has 4 perceptrons with 15 weights each, 2nd layer has 2 perceptrons with 4 weights each, output layer has 1 perceptron with 2 weights, and all of the layers have 1 bais term each\n\n**Calculating Predictions** requires a slight change, looping over all layers, multiply the input values with weights of each layer. \n\n$$output = input * weights + bais$$\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndef calc_preds(coeff, indep_var):\n    layers, consts = coeff\n    n = len(layers)\n    res = indep_var\n    for i, l in enumerate(layers[: -1]):\n        res = F.relu( res.matmul(layers[i]) + consts[i] )\n        # relu activation for all hidden layers\n    res = res.matmul(layers[-1]) + consts[-1]\n    return torch.sigmoid(res)                    # sigmoid for the output layer\n```\n:::\n\n\n**Updating Coefficients** will simply loop over all coeff and baises and subtracting $graddient * learningRate$\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndef update_coeff(coeff, lr):\n    layers, consts = coeff\n    for layer in (layers+consts):\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n```\n:::\n\n\n**Train Function** will take input of coeff as `list` instead of `int`\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ndef train_model(n_epochs, lr, n_hidden=[10,10]):\n    coeff = init_coeffs(n_hidden, X)\n    for i in range(n_epochs):\n        one_epoch(coeff, lr)\n    print('\\n\\nFinal Loss: ', float(calc_loss(coeff, X, y)))\n    return coeff\n```\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ncoeff = train_model(40, 2, [10,5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n loss : 0.495 ; loss : 0.436 ; loss : 0.367 ; loss : 0.353 ; loss : 0.335 ; loss : 0.316 ; loss : 0.307 ; loss : 0.307 ; loss : 0.305 ; loss : 0.267 ; loss : 0.261 ; loss : 0.237 ; loss : 0.228 ; loss : 0.223 ; loss : 0.225 ; loss : 0.213 ; loss : 0.212 ; loss : 0.214 ; loss : 0.204 ; loss : 0.202 ; loss : 0.201 ; loss : 0.2 ; loss : 0.199 ; loss : 0.199 ; loss : 0.198 ; loss : 0.197 ; loss : 0.196 ; loss : 0.195 ; loss : 0.194 ; loss : 0.194 ; loss : 0.193 ; loss : 0.193 ; loss : 0.193 ; loss : 0.192 ; loss : 0.192 ; loss : 0.192 ; loss : 0.192 ; loss : 0.192 ; loss : 0.191 ; loss : 0.191 ;\n\nFinal Loss:  0.19121552750666765\n```\n:::\n:::\n\n\n# Tada üéâüéâüéâ !!!\n\nNow you know how to build a complete Neural Network from scratch !!!\n\n\nLet me know if I missed something. üòâ\n\n#### Where to go from here? \nüòè Haah! **Nowhere !!!** \n\n<br>\n</br>\n<br>\n\n**PS :**\n\n\n> For the flowcharts, I have used the tool named mermaid, you can generate them easily in markdown.\n\n> To transform this notebook into webpage, I have used Quarto.\n\n> My only recommendation - do fast.ai course üòå\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}